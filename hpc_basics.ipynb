{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPDCkaAMHEtVnP5i0b8X/u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacobDowns/CSCI-491-591/blob/main/hpc_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HPC and SLURM Basics\n",
        "* Now that we're setup on Anvil let's go a couple of HPC / SLURM basics\n",
        "* For reference here's the Anvil [user guide](https://www.rcac.purdue.edu/knowledge/anvil)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gxoD5tX3pycq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linux Preliminaries\n",
        "\n",
        "**Navigation**\n",
        "\n",
        "| Command | Description | Example |\n",
        "|----------|--------------|----------|\n",
        "| `pwd` | Print current working directory | `pwd` → `/home/username` |\n",
        "| `ls` | List files and directories | `ls -l`, `ls -lh` |\n",
        "| `cd` | Change directory | `cd /projects/cis250773` |\n",
        "| `cd ..` | Move up one directory |  |\n",
        "| `cd ~` | Go to home directory |  |\n",
        "| `tree` | Display directory tree | `tree -L 2` |\n",
        "| `du -sh *` | Show size of each folder | Useful for checking quotas |\n"
      ],
      "metadata": {
        "id": "-7t0G1qhqiPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Managing Files and Directories**\n",
        "| Command | Description | Example |\n",
        "|----------|--------------|----------|\n",
        "| `mkdir` | Create a new directory | `mkdir data`, `mkdir -p results/run1` |\n",
        "| `cp` | Copy files or directories | `cp input.txt /scratch/$USER/` |\n",
        "| `mv` | Move or rename files | `mv old.txt new.txt` |\n",
        "| `rm` | Remove a file | `rm temp.txt` |\n",
        "| `rm -r` | Remove directory recursively | `rm -r old_results/` ⚠️ irreversible |\n",
        "| `touch` | Create an empty file | `touch notes.txt` |\n",
        "| `cat` | Print file contents | `cat job.o12345` |\n",
        "| `head` / `tail` | Show first/last lines of a file | `head -n 10 log.txt`, `tail -f log.txt` |\n",
        "| `nano`, `vi`, `vim` | Edit files in terminal | `nano test.slurm` |\n",
        "| `chmod` | Change file permissions | `chmod +x script.sh` |"
      ],
      "metadata": {
        "id": "WhM_-oJEre9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking Disk Usage / Quotas**\n",
        "| Command | Description | Example |\n",
        "|----------|--------------|----------|\n",
        "| `df -h` | Show filesystem disk usage |  |\n",
        "| `du -sh .` | Show total size of current directory |  |\n",
        "| `du -sh *` | Show size of all subdirectories |  |\n"
      ],
      "metadata": {
        "id": "k043A3pArw5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HPC Cluster Preliminaries\n",
        "* HPC clusters will typically include a few specialized types of nodes\n",
        "* Login Node\n",
        "  * Where you login, set up environments, submit jobs\n",
        "* Compute nodes\n",
        "  * Where jobs actually run\n",
        "  * This is where the heavy computation actually happens\n",
        "  * **Don't do heavy computation on login nodes!**\n",
        "* Storage nodes\n",
        "  * Where shared filesystems live like your home directory and projects"
      ],
      "metadata": {
        "id": "IwiFPT97uka9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Slurm\n",
        "* Slurm is the Simple Linux Utility for Resource Management\n",
        "* It's the job scheduler on the cluster that coordinates how resources are used\n",
        "* It controls\n",
        "  * Who runs based on queue order, priority, and allocation\n",
        "  * Where they run (which nodes)\n",
        "  * When they run (once resources are free)\n",
        "* Hence, you don't directly run something on a compute node, you submit a request for resources with Slurm"
      ],
      "metadata": {
        "id": "K6-m4NWP8hML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Managing environments\n",
        "* To do computation on a compute node, you'll use Slurm with commands like `sbatch` and `salloc`\n",
        "* To support a wide variety of software, HPC systems often use **modules** that allow you to setup a particular software environment\n",
        "* Compute nodes start with a minimal clean Linux environment\n",
        "* To start out with, nothing you loaded or activated interactively on the login node carries over\n",
        "\n"
      ],
      "metadata": {
        "id": "El2zA-cmwTIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modules\n",
        "* Modules on an HPC cluster are configuration files that dynamically set up your shell environment to use a particular version of a software\n",
        "* Modules are dynamic, so they can be loaded and unloaded each session\n",
        "* Some examples of module related commands:\n",
        "```\n",
        "module avail            # list available modules\n",
        "module load gcc/11.2.0  # load GCC 11.2.0\n",
        "module list             # show currently loaded modules\n",
        "module unload gcc/11.2.0\n",
        "module purge    # unload everything\n",
        "```\n",
        "* Note that these modules modify your environment only temporarily\n",
        "* When you log back in, these will disappear\n",
        "* The module system enables many different users to work with different software versions on the same cluster"
      ],
      "metadata": {
        "id": "d9uYJa-OjBib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up Environments\n",
        "* Often you will setup an environment on the login node, then explicitly recreate the envionrment in your job script\n",
        "* Hence, on the login node, you might:\n",
        "  * Create an anaconda environment of virtual environment\n",
        "  * Compile software\n",
        "  * Manage files\n",
        "* To run code on the compute node, you'll need to recreate the same environment on the compute node\n",
        "* Compute nodes have access to the shared file system\n",
        "  * For example, in Anvil, compute nodes can access your home directory, scratch, and the projects directory"
      ],
      "metadata": {
        "id": "VPxPHakmi_Ne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example: Numba-CUDA Environment\n",
        "* You can set up an environment to run Numba CUDA on Anvil by first setting up an Anaconda environment on the login node in your home directory\n",
        "\n",
        "```\n",
        "module purge\n",
        "module load modtree/gpu\n",
        "module load anaconda\n",
        "conda create -y -n numba-cuda -c conda-forge python=3.11 numba cudatoolkit\n",
        "```"
      ],
      "metadata": {
        "id": "MTXBfMdSzjEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* As an example, let's say we want to run this Numba CUDA code for doing vector addition on Anvil\n",
        "```python\n",
        "import numpy as np\n",
        "from numba import cuda\n",
        "\n",
        "@cuda.jit\n",
        "def vec_add(a, b, c):\n",
        "    i = cuda.grid(1)\n",
        "    if i < a.size:\n",
        "        c[i] = a[i] + b[i]\n",
        "\n",
        "def main():\n",
        "    # Print device info\n",
        "    dev = cuda.get_current_device()\n",
        "    print(f\"Using GPU: {dev.name.decode() if hasattr(dev.name, 'decode') else dev.name}\")\n",
        "    print(f\"Compute capability: {dev.compute_capability}\")\n",
        "    print(f\"Max threads per block: {dev.MAX_THREADS_PER_BLOCK}\")\n",
        "\n",
        "    # Problem size\n",
        "    n = 1_000_000\n",
        "    a = np.random.rand(n).astype(np.float32)\n",
        "    b = np.random.rand(n).astype(np.float32)\n",
        "    c = np.empty_like(a)\n",
        "\n",
        "    # Configure kernel launch\n",
        "    threads_per_block = 256\n",
        "    blocks = (n + threads_per_block - 1) // threads_per_block\n",
        "\n",
        "    # Move data to device\n",
        "    d_a = cuda.to_device(a)\n",
        "    d_b = cuda.to_device(b)\n",
        "    d_c = cuda.device_array_like(a)\n",
        "\n",
        "    # Launch kernel\n",
        "    vec_add[blocks, threads_per_block](d_a, d_b, d_c)\n",
        "\n",
        "    # Copy result back and verify\n",
        "    d_c.copy_to_host(c)\n",
        "    max_err = np.max(np.abs(c - (a + b)))\n",
        "    print(f\"Max error: {max_err:.3e}\")\n",
        "    print(\"SUCCESS\" if max_err < 1e-6 else \"CHECK FAILED\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "```"
      ],
      "metadata": {
        "id": "wasQ39_c13Yh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* To run this test script, we define a SLURM file that activates the environment we set up and executes the script\n",
        "\n",
        "```\n",
        "#!/bin/bash\n",
        "# FILENAME: numba_cuda.slurm\n",
        "\n",
        "#SBATCH -A cis250773-gpu          # your GPU allocation\n",
        "#SBATCH --nodes=1\n",
        "#SBATCH --ntasks-per-node=1\n",
        "#SBATCH --gpus-per-node=1\n",
        "#SBATCH --time=0:5:00\n",
        "#SBATCH -J numba_cuda\n",
        "#SBATCH -o numba_cuda.o%j\n",
        "#SBATCH -e numba_cuda.e%j\n",
        "#SBATCH -p gpu\n",
        "\n",
        "module purge\n",
        "module load modtree/gpu\n",
        "module load anaconda              \n",
        "\n",
        "# Activate the environment you created in step 1\n",
        "conda activate numba-cuda\n",
        "\n",
        "# Run the program\n",
        "python numba_cuda_test.py\n",
        "```\n",
        "* Note that the compute node has access to the same file system as the login node so we can access the conda environment we set up on the login node\n",
        "* Loading the same modules ensures that everything is consistent"
      ],
      "metadata": {
        "id": "oOVxdjIp5bPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This job can be submitted witht the command\n",
        "```\n",
        "sbatch numba_cuda.slurm\n",
        "```\n",
        "* Note that you'll most likely want to keep your job scripts in your home directory"
      ],
      "metadata": {
        "id": "hXIrUtiS9FtS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ways to use Slurm\n",
        "1. As shown above, the main way of using SLURM is by submitting a job with\n",
        "```\n",
        "sbatch my_job.slurm\n",
        "```\n",
        "* This will cause Slurm to queue the job and eventually run it\n",
        "2. The second way of using Slurm is in interactive mode with a command like\n",
        "```\n",
        "salloc -A your-account -p gpu --gpus=1 --time=30:00\n",
        "```\n",
        "* This allows you to run commands manually, which can be useful for debugging / experimenting to make sure your environment is properly setup on a compute node"
      ],
      "metadata": {
        "id": "bDBX3DV485ZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Scripts\n",
        "* There are some common flags that will often apear in batch scripts for Slurm jobs\n",
        "\n",
        "| Category | Flag | Description | Example |\n",
        "|-----------|------|--------------|----------|\n",
        "| **Job Identification** | `#SBATCH -J <name>` | Sets a short, descriptive job name | `#SBATCH -J gpu_test` |\n",
        "|  | `#SBATCH -o <file>` | File for standard output (`stdout`) | `#SBATCH -o job.o%j` |\n",
        "|  | `#SBATCH -e <file>` | File for standard error (`stderr`) | `#SBATCH -e job.e%j` |\n",
        "|  | `#SBATCH -D <path>` | Set working directory for the job | `#SBATCH -D /home/username/jobs` |\n",
        "| **Resources** | `#SBATCH -A <account>` | Allocation or project account to charge | `#SBATCH -A cis250773-gpu` |\n",
        "|  | `#SBATCH -p <partition>` | Queue/partition to submit to | `#SBATCH -p gpu` |\n",
        "|  | `#SBATCH --nodes=<n>` | Number of nodes requested | `#SBATCH --nodes=1` |\n",
        "|  | `#SBATCH --ntasks-per-node=<n>` | Number of tasks (processes) per node | `#SBATCH --ntasks-per-node=1` |\n",
        "|  | `#SBATCH --cpus-per-task=<n>` | Number of CPU cores per task | `#SBATCH --cpus-per-task=4` |\n",
        "|  | `#SBATCH --gpus-per-node=<n>` | Number of GPUs per node | `#SBATCH --gpus-per-node=1` |\n",
        "|  | `#SBATCH --mem=<size>` | Amount of memory per node or per job | `#SBATCH --mem=16G` |\n",
        "| **Time Management** | `#SBATCH --time=<hh:mm:ss>` | Walltime limit for the job | `#SBATCH --time=02:00:00` |\n",
        "|  | `#SBATCH --begin=<time>` | Delay start until specific time | `#SBATCH --begin=tomorrow` |\n",
        "| **Email Notifications** | `#SBATCH --mail-user=<email>` | Email address for job notifications | `#SBATCH --mail-user=username@purdue.edu` |\n",
        "|  | `#SBATCH --mail-type=<type>` | When to send emails (`BEGIN`, `END`, `FAIL`, etc.) | `#SBATCH --mail-type=END,FAIL` |\n",
        "| **Job Arrays** | `#SBATCH --array=<range>` | Submit multiple similar jobs as an array | `#SBATCH --array=1-10` |\n",
        "| **Dependencies & Advanced** | `#SBATCH --dependency=<type:jobid>` | Start job only after another finishes | `#SBATCH --dependency=afterok:12345` |\n",
        "|  | `#SBATCH --requeue` | Allow job to be requeued if preempted | `#SBATCH --requeue` |\n",
        "| **Miscellaneous** | `#SBATCH --export=<vars>` | Export environment variables to the job | `#SBATCH --export=ALL` |\n",
        "|  | `#SBATCH --chdir=<path>` | Change directory before running job | `#SBATCH --chdir=/projects/cis250773/` |\n",
        "|  | `#SBATCH --constraint=<feature>` | Request nodes with a specific feature | `#SBATCH --constraint=zen3` |\n"
      ],
      "metadata": {
        "id": "lUmM88YAGL4h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Notes:**\n",
        "> - `%j` in filenames (like `job.o%j`) is replaced by the **job ID** automatically.  \n",
        "> - You can check available partitions and resources with `sinfo` or the cluster’s documentation.  \n",
        "> - Use `--time` conservatively — Slurm will **kill the job** when the limit expires.  \n",
        "> - Email notifications only work if the cluster’s mail system is configured for users.  \n",
        "> - For interactive testing, use `salloc` or `srun` instead of a batch script."
      ],
      "metadata": {
        "id": "OAmijgXmG-qz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Along with these flags, Slurm has many command line arguments that are used to do things like submit jobs, view jobs in the queue, and so on\n",
        "* These are useful if you primarily want to interact with the cluster from the command line as opposed to the OnDemand interface\n",
        "\n",
        "| Command | Description | Common Usage Example |\n",
        "|----------|--------------|----------------------|\n",
        "| **`sbatch`** | Submit a batch script to the Slurm scheduler | `sbatch job.slurm` |\n",
        "| **`squeue`** | View the queue of running and pending jobs | `squeue -u $USER` |\n",
        "| **`scancel`** | Cancel a submitted job | `scancel 123456` |\n",
        "| **`sacct`** | Display accounting data for completed jobs | `sacct -j 123456` |\n",
        "| **`sinfo`** | Show partition and node availability | `sinfo -p gpu` |\n",
        "| **`scontrol`** | View or modify job and node information | `scontrol show job 123456` |\n",
        "| **`salloc`** | Allocate resources for an interactive job | `salloc -A cis250773 -p gpu --gpus=1 --time=30:00` |\n",
        "| **`srun`** | Run a command or program under Slurm control | `srun python script.py` |\n",
        "| **`sstat`** | Show real-time status for a running job | `sstat -j 123456.batch` |\n",
        "| **`sprio`** | Display job priorities in the queue | `sprio` |\n",
        "| **`squeue --start`** | Estimate job start times | `squeue --start -u $USER` |\n",
        "| **`sreport`** | Generate usage and efficiency reports | `sreport cluster utilization` |\n",
        "| **`sview`** | Open a GUI for job/cluster monitoring (if available) | `sview` |\n",
        "\n",
        "\n",
        "> **Notes:**\n",
        "> - `$USER` is a built-in environment variable for your username.  \n",
        "> - Job states in `squeue`:  \n",
        ">   - `PD` = Pending  \n",
        ">   - `R` = Running  \n",
        ">   - `CG` = Completing  \n",
        ">   - `CD` = Completed  \n",
        ">   - `F` = Failed  \n",
        "> - Use `sacct` after a job finishes to check CPU, memory, and walltime usage.  \n",
        "> - For quick testing, `salloc` + `srun` gives you an interactive shell or command on a compute node.\n",
        "\n"
      ],
      "metadata": {
        "id": "FnR_lPuVHRSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Storing and Managing Files\n",
        "* Anvil has a few different file systems that you can use for different purposes\n",
        "* You can see the main ones using the `myquota` command on Anvil\n",
        "```\n",
        "myquota\n",
        "\n",
        "Type       Location             Size    Limit    Use   Files   Limit    Use\n",
        "===========================================================================\n",
        "home       x-jdowns            3.0GB   25.0GB  12.0%      -       -      -\n",
        "scratch    x-jdowns            0.0KB  100.0TB   0.0%      1     1.0M   0.0%\n",
        "projects   x-cis250773         2.8GB    5.0TB   0.1%   10.5K    1.0M   1.0%\n",
        "```\n",
        "* The user's **home** directory can be used to store personal software scripts etc.\n",
        "* **scratch** is temporary user storage for I/O activity\n",
        "  * It's useful for fast read / write access to datasets\n",
        "  * However, it's not long term storage!\n",
        "  * It is purged after 30 days (access time)\n",
        "* **projects** is a shared, per allocation storage space for common datasets and software installation\n"
      ],
      "metadata": {
        "id": "LDDPLu_XI9zB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### File Transfer\n",
        "* There are a few different ways of handling file transfers from your system to a cluster\n",
        "* One is to use the OnDemand file browser interface\n",
        "* There are also command line tools like `scp` and `rsync`\n",
        "* You can even set up VSCode for remote SSH access so you can edit files on the cluster directly in VSCode\n",
        "* You can also use `git` to clone code repositiories\n",
        "\n",
        "\n",
        "| Command | Description | Example |\n",
        "|----------|--------------|----------|\n",
        "| `scp` | Secure copy between local and remote | `scp file.txt scp FDS-6.10.1_SMV-6.10.1_lnx.sh x-jdowns@login07.anvil.rcac.purdue.edu:/home/x-jdowns` |\n",
        "| `rsync` | Sync directories efficiently | `rsync -av data/ x-jdowns@login07.anvil.rcac.purdue.edu::~/data_backup/` |\n",
        "| `sftp` | Interactive file transfer | `sftp x-jdowns@login07.anvil.rcac.purdue.edu:` |\n",
        "| (GUI) | File transfer via app | FileZilla, VSCode or WinSCP, OnDemand UI |"
      ],
      "metadata": {
        "id": "OVERBDqqJ6WS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Containers\n",
        "\n",
        "* Using modules or Conda environments will probably suffice for many use cases\n",
        "* However, sometimes the software you want to use on the HPC cluster is difficult (or impossible) to setup with the module system\n",
        "* A useful alternative to using modules is to use **containers**"
      ],
      "metadata": {
        "id": "6yiUuvPb2WoS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **What is a container?**\n",
        "  * A lightweight, isolated user-space that packages an app and its dependencies\n",
        "  * A container is a bit like a lightweight virutal machine (VM)\n",
        "  * Docker is perhaps most commonly used tool for containerization\n",
        "  * In HPC, we typically use Apptainer (the open successor to Singularity) to run containers—often built as Docker images elsewhere\n",
        "* Anvil has **Apptainer** installed by default\n",
        "  * Apptainer can pull Docker images and convert them to .sif files it can use\n",
        "> Many common software packages have prebuilt containers!\n",
        "* **Why containers on HPC?**\n",
        "  * Reproducibility: exact versions of libs/tools\n",
        "  * Portability: same image runs on laptop, cloud, HPC\n",
        "  * Stability: avoid conflicts with cluster software"
      ],
      "metadata": {
        "id": "4v-oNH4s5edQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: Fire Dynamics Simulator\n",
        "* The Fire Dynamics Simulator is a computational fluid dynamics (CFD) model of fire-driven fluid flow\n",
        "* It is an extremely sophisticated but computationally demanding model\n",
        "* The easiest way to use FDS is via prebuilt binaries\n",
        "* However, this leads to conflicts with some of Anvil's system software\n",
        "* We could alternatively compile FDS from source\n",
        "* However, it's easier to use an existing container!\n",
        "> **Note:** While I don't expect most of you are interested in using FDS, this demonstrates how you can setup a container based workflow."
      ],
      "metadata": {
        "id": "woZ0KhdTDJTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Walkthrough\n",
        "Here we'll walkthrough how you can run the FDS model on Anvil to demonstrate the use of containers.\n",
        "\n",
        "1. On the shell in Anvil you can get the installed version of apptainer with\n",
        "```\n",
        "apptainer --version\n",
        "```\n",
        "\n",
        "2. Apptainer allows you to pull docker images and converts them to the correct format automatically. For example, you can pull a container that has FDS installed with\n",
        "```\n",
        "apptainer pull fds.sif docker://openbcl/fds\n",
        "```\n",
        "\n",
        "> **Note**: This container already exists in /anvil/projects/x-cis250773/containers. You can reuse this container in this walkthrough.\n"
      ],
      "metadata": {
        "id": "6jMVDAJvDqTA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can open an anteractive shell in the container using the command\n",
        "```\n",
        "apptainer shell /anvil/projects/x-cis250773/containers/fds.sif\n",
        "```\n",
        "In the shell type in `fds` to get information about the FDS version installed in the container. You can exit out of the shell with the `exit` command."
      ],
      "metadata": {
        "id": "OK8v5jesE0ai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input File\n",
        "Now that we have the container, we need to setup an example simulation to run. You can use the following simulation setup that creates a simple box of fuel that will be burned.\n",
        "\n",
        "> **Note:** You can find this already in /anvil/projects/x-cis250773/fds_tests/box_burn_away1.fds\n",
        "\n",
        "```\n",
        "&HEAD CHID='box_burn_away1', TITLE='Test BURN_AWAY feature' /\n",
        "\n",
        "The FOAM box is evaporated away by the high thermal radiation\n",
        "from HOT surfaces. The mass of the box is 0.4^3 m3 * 20 kg/m3 = 1.28 kg.\n",
        "This should be compared to the final value of fuel density volume integral,\n",
        "computed by the first DEVC.\n",
        "\n",
        "&MESH IJK=10,10,10 XB=-0.3,0.7,-0.4,0.6,0.0,1.0, MULT_ID='mesh' /\n",
        "&MULT ID='mesh', DX=1.0, DY=1.0, I_UPPER=1, J_UPPER=1 /\n",
        "\n",
        "&TIME T_END=30. DT = 0.01/\n",
        "\n",
        "&SPEC ID='METHANE' /\n",
        "\n",
        "&MATL ID                   = 'FOAM'\n",
        "      HEAT_OF_REACTION     = 800.\n",
        "      CONDUCTIVITY         = 0.2\n",
        "      SPECIFIC_HEAT        = 1.0\n",
        "      DENSITY              = 20.\n",
        "      NU_SPEC              = 1.\n",
        "      SPEC_ID              = 'METHANE'\n",
        "      REFERENCE_TEMPERATURE= 200. /\n",
        "\n",
        "&SURF ID                   = 'FOAM SLAB'\n",
        "      COLOR                = 'TOMATO 3'\n",
        "      VARIABLE_THICKNESS   = T\n",
        "      BURN_AWAY            = T /\n",
        "\n",
        "&OBST XB=0.30,0.70,0.30,0.70,0.30,0.70, SURF_ID='FOAM SLAB', BULK_DENSITY=20., MATL_ID='FOAM' /\n",
        "\n",
        "&SURF ID='HOT', TMP_FRONT=1100., COLOR='RED' /\n",
        "\n",
        "&VENT PBX=-0.3, SURF_ID='HOT' /\n",
        "&VENT PBX= 1.7, SURF_ID='HOT' /\n",
        "&VENT PBY=-0.4, SURF_ID='HOT' /\n",
        "&VENT PBY= 1.6, SURF_ID='HOT' /\n",
        "&VENT PBZ= 0.0, SURF_ID='HOT' /\n",
        "&VENT PBZ= 1.0, SURF_ID='HOT' /\n",
        "\n",
        "&BNDF QUANTITY='WALL TEMPERATURE' /\n",
        "&BNDF QUANTITY='MASS FLUX', SPEC_ID='METHANE' /\n",
        "\n",
        "&SLCF PBZ=0.5, QUANTITY='TEMPERATURE', CELL_CENTERED=T /\n",
        "&SLCF PBZ=0.5, QUANTITY='DENSITY', CELL_CENTERED=T /\n",
        "&SLCF PBZ=0.5, QUANTITY='MASS FRACTION', SPEC_ID='METHANE', CELL_CENTERED=T /\n",
        "\n",
        "&DEVC XB=-0.3,1.7,-0.4,1.6,0,1, QUANTITY='DENSITY', SPEC_ID='METHANE', SPATIAL_STATISTIC='VOLUME INTEGRAL', ID='Mass fuel' /\n",
        "\n",
        "&TAIL /\n",
        "```"
      ],
      "metadata": {
        "id": "azIZMLOtHG-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the Slurm Job\n",
        "Next, we need to setup the Slurm batch script. The contents of the slurm box will look like this.\n",
        "\n",
        "> **Note:** You'll need to modify the `CASE_DIR` to output to your scratch or user directory, or a custom directory in the shared project folder. You can find a version of this file in /anvil/projects/x-cis250773/fds_tests/run_fds.slurm\n",
        "\n",
        "\n",
        "```\n",
        "#!/bin/bash\n",
        "#SBATCH -A cis250773\n",
        "#SBATCH -p shared\n",
        "#SBATCH -J fds_box_output\n",
        "#SBATCH --time=00:10:00\n",
        "#SBATCH -o fds_box_output.o%j\n",
        "#SBATCH -e fds_box_output.e%j\n",
        "#SBATCH --nodes=1\n",
        "#SBATCH --ntasks=1\n",
        "#SBATCH --cpus-per-task=1\n",
        "#SBATCH --mem=4G\n",
        "\n",
        "set -euo pipefail\n",
        "\n",
        "# --- Configuration ---\n",
        "IMG=\"/anvil/projects/x-cis250773/containers/fds.sif\"\n",
        "CASE_DIR=\"your own personal output directory here!\"\n",
        "INPUT_FILE=\"box_burn_away1.fds\"\n",
        "\n",
        "# Create a unique output directory\n",
        "OUT_DIR=\"${CASE_DIR}/output/run_${SLURM_JOB_ID}\"\n",
        "mkdir -p \"$OUT_DIR\"\n",
        "\n",
        "echo \"Starting FDS job $SLURM_JOB_ID\"\n",
        "echo \"Case directory : $CASE_DIR\"\n",
        "echo \"Output directory: $OUT_DIR\"\n",
        "\n",
        "# Copy the input file into the output dir (optional, for reproducibility)\n",
        "cp \"${CASE_DIR}/${INPUT_FILE}\" \"$OUT_DIR/\"\n",
        "\n",
        "# Run FDS inside the container\n",
        "apptainer exec --bind \"$OUT_DIR:$OUT_DIR\" --pwd \"$OUT_DIR\" \"$IMG\" \\\n",
        "  fds \"$INPUT_FILE\"\n",
        "\n",
        "# --- Post-processing ---\n",
        "echo \"Simulation completed. Output files are in: $OUT_DIR\"\n",
        "echo \"Contents:\"\n",
        "ls -lh \"$OUT_DIR\"\n",
        "\n",
        "# You can rename/move summary logs if you like\n",
        "mv \"fds_box_output.o${SLURM_JOB_ID}\" \"$OUT_DIR/\" 2>/dev/null || true\n",
        "mv \"fds_box_output.e${SLURM_JOB_ID}\" \"$OUT_DIR/\" 2>/dev/null || true\n",
        "```"
      ],
      "metadata": {
        "id": "9_ZhQyAUH1Z_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main command to look at here is\n",
        "```\n",
        "apptainer exec --bind \"$OUT_DIR:$OUT_DIR\" --pwd \"$OUT_DIR\" \"$IMG\" \\\n",
        "  fds \"$INPUT_FILE\"\n",
        "```\n",
        "\n",
        "* The exec command is going to tell apptainer to execute a command inside the container.\n",
        "* The `--bind \"$OUT_DIR:$OUT_DIR\"` argument mounts a directory from the host file system into the container using the same name for both.\n",
        "* The agrument `--pwd \"$OUT_DIR\"` sets the working directory in the container.\n",
        "* `$IMG$` is the path to the container image.\n",
        "* `fds $INPUT_FILE` runs the `fds` command in the container with the given input file"
      ],
      "metadata": {
        "id": "586DhGLTJ37u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running\n",
        "Once you've created an modified your own version of this file, navigate to the directory you specified in `$CASE_DIR` and submit the job as normal:\n",
        "```\n",
        "sbatch run_fds_container.slurm\n",
        "```\n",
        "Pay attention to the output once the job begins by inspecting the standard output and error files. What is being printed out? What files does FDS write?"
      ],
      "metadata": {
        "id": "AubjLxUWJJRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing\n",
        "* If you want to see the results of the simulation your ran, you can do so using an application called Smokeview\n",
        "* You can download SmokeView for different platforms [here](https://pages.nist.gov/fds-smv/downloads.html)\n",
        "* The easiest way to visualize the output is to install Smokeview on your local machine and transfer the `box_burn_away1.smv` file to your machine\n",
        "* This file can be inspected in Smokeview"
      ],
      "metadata": {
        "id": "7qkUPcBDMSx7"
      }
    }
  ]
}